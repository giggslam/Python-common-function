{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何用sklearn計算中文文本TF-IDF\n",
    "2019-02-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 語料集\n",
    "本文所用語料集為人機對話系統中的短文本語料，corpus列表中的每個元素是一條Query。（如果是长文本的话，每个元素是一篇文档）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "  \"幫我 查下 明天 北京 天氣 怎麼樣\",\n",
    "  \"幫我 查下 今天 北京 天氣 好不好\",\n",
    "  \"幫我 查詢 去 北京 的 火車\",\n",
    "  \"幫我 查看 到 上海 的 火車\",\n",
    "  \"幫我 查看 特朗普 的 新聞\",\n",
    "  \"幫我 看看 有沒有 北京 的 新聞\",\n",
    "  \"幫我 搜索 上海 有 什麼 好玩的\",\n",
    "  \"幫我 找找 上海 東方明珠 在哪\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 將語料轉換為詞袋向量\n",
    "\n",
    "### step 1. 聲明一個向量化工具vectorizer\n",
    "\n",
    "本文使用的是CountVectorizer，默認情況下，CountVectorizer僅統計長度超過兩個字符的詞，但是在短文本中任何一個字都可能十分重要，比如“去／到”等，所以要想讓CountVectorizer也支持單字符的詞，需要加上參數token_pattern='\\\\b\\\\w+\\\\b'。\n",
    "\n",
    "### step 2.根據語料集統計詞袋（fit）；\n",
    "\n",
    "### step 3.打印語料集的詞袋信息；\n",
    "\n",
    "### step 4.將語料集轉化為詞袋向量（transform）；\n",
    "\n",
    "### step 5.還可以查看每個詞在詞袋中的索引；\n",
    "\n",
    "### step 6.將詞袋向量轉化為 numpy array，方便之後比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "['上海', '什麼', '今天', '到', '北京', '去', '在哪', '天氣', '好不好', '好玩的', '幫我', '怎麼樣', '找找', '搜索', '新聞', '明天', '有', '有沒有', '東方明珠', '查下', '查看', '查詢', '火車', '特朗普', '的', '看看']\n",
      "26\n",
      "Vectorized corpus:\n",
      "[[0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "index of `什麼` is : 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# step 1\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=1.0, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# step 2\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# step 3\n",
    "bag_of_words = vectorizer.get_feature_names()\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_words)\n",
    "print(len(bag_of_words))\n",
    "\n",
    "# step 4\n",
    "X = vectorizer.transform(corpus)\n",
    "print(\"Vectorized corpus:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Step 4.5\n",
    "# Step 3&4 can be combined into Step 4.5\n",
    "# fit + transform = fit_transform\n",
    "#Y = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# step 5\n",
    "print(\"index of `什麼` is : {}\".format(vectorizer.vocabulary_.get('什麼')))\n",
    "\n",
    "# step 6\n",
    "corpus_vecs = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x26 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 19)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 19)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 24)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 20)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 24)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 14)\t1\n",
      "  (4, 20)\t1\n",
      "  (4, 23)\t1\n",
      "  (4, 24)\t1\n",
      "  (5, 4)\t1\n",
      "  (5, 10)\t1\n",
      "  (5, 14)\t1\n",
      "  (5, 17)\t1\n",
      "  (5, 24)\t1\n",
      "  (5, 25)\t1\n",
      "  (6, 0)\t1\n",
      "  (6, 1)\t1\n",
      "  (6, 9)\t1\n",
      "  (6, 10)\t1\n",
      "  (6, 13)\t1\n",
      "  (6, 16)\t1\n",
      "  (7, 0)\t1\n",
      "  (7, 6)\t1\n",
      "  (7, 10)\t1\n",
      "  (7, 12)\t1\n",
      "  (7, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 根據詞袋向量統計TF-IDF\n",
    "### step 1.聲明一個TF-IDF轉化器（TfidfTransformer）；\n",
    "### step 2.根據語料集的詞袋向量計算TF-IDF（fit）；\n",
    "### step 3.打印TF-IDF信息：比如結合詞袋信息，可以查看每個詞的TF-IDF值；\n",
    "### step 4.將語料集的詞袋向量表示轉換為TF-IDF向量表示；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海\t1.8109302162163288\n",
      "什麼\t2.504077396776274\n",
      "今天\t2.504077396776274\n",
      "到\t2.504077396776274\n",
      "北京\t1.587786664902119\n",
      "去\t2.504077396776274\n",
      "在哪\t2.504077396776274\n",
      "天氣\t2.09861228866811\n",
      "好不好\t2.504077396776274\n",
      "好玩的\t2.504077396776274\n",
      "幫我\t1.0\n",
      "怎麼樣\t2.504077396776274\n",
      "找找\t2.504077396776274\n",
      "搜索\t2.504077396776274\n",
      "新聞\t2.09861228866811\n",
      "明天\t2.504077396776274\n",
      "有\t2.504077396776274\n",
      "有沒有\t2.504077396776274\n",
      "東方明珠\t2.504077396776274\n",
      "查下\t2.09861228866811\n",
      "查看\t2.09861228866811\n",
      "查詢\t2.504077396776274\n",
      "火車\t2.09861228866811\n",
      "特朗普\t2.504077396776274\n",
      "的\t1.587786664902119\n",
      "看看\t2.504077396776274\n",
      "[[0.         0.         0.         0.         0.3183848  0.\n",
      "  0.         0.42081614 0.         0.         0.20052115 0.50212047\n",
      "  0.         0.         0.         0.50212047 0.         0.\n",
      "  0.         0.42081614 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.50212047 0.         0.3183848  0.\n",
      "  0.         0.42081614 0.50212047 0.         0.20052115 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42081614 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33116919 0.52228256\n",
      "  0.         0.         0.         0.         0.20857285 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.52228256 0.43771355 0.\n",
      "  0.33116919 0.        ]\n",
      " [0.38715525 0.         0.         0.53534183 0.         0.\n",
      "  0.         0.         0.         0.         0.21378805 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.44865824 0.         0.44865824 0.\n",
      "  0.33944982 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23187059 0.\n",
      "  0.         0.         0.48660646 0.         0.         0.\n",
      "  0.         0.         0.48660646 0.         0.         0.5806219\n",
      "  0.36816103 0.        ]\n",
      " [0.         0.         0.         0.         0.33116919 0.\n",
      "  0.         0.         0.         0.         0.20857285 0.\n",
      "  0.         0.         0.43771355 0.         0.         0.52228256\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33116919 0.52228256]\n",
      " [0.33420711 0.4621274  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4621274  0.18454996 0.\n",
      "  0.         0.4621274  0.         0.         0.4621274  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.37686288 0.         0.         0.         0.         0.\n",
      "  0.52110999 0.         0.         0.         0.20810458 0.\n",
      "  0.52110999 0.         0.         0.         0.         0.\n",
      "  0.52110999 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# step 1\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# step 2\n",
    "tfidf_transformer.fit(X.toarray())\n",
    "\n",
    "# step 3\n",
    "for idx, word in enumerate(vectorizer.get_feature_names()):\n",
    "    print(\"{}\\t{}\".format(word, tfidf_transformer.idf_[idx]))\n",
    "\n",
    "# step 4\n",
    "tfidf = tfidf_transformer.transform(X)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x26 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19)\t0.4208161427072783\n",
      "  (0, 15)\t0.502120471152222\n",
      "  (0, 11)\t0.502120471152222\n",
      "  (0, 10)\t0.2005211467499556\n",
      "  (0, 7)\t0.4208161427072783\n",
      "  (0, 4)\t0.3183848028404604\n",
      "  (1, 19)\t0.4208161427072783\n",
      "  (1, 10)\t0.2005211467499556\n",
      "  (1, 8)\t0.502120471152222\n",
      "  (1, 7)\t0.4208161427072783\n",
      "  (1, 4)\t0.3183848028404604\n",
      "  (1, 2)\t0.502120471152222\n",
      "  (2, 24)\t0.3311691914996199\n",
      "  (2, 22)\t0.4377135482191675\n",
      "  (2, 21)\t0.5222825618037251\n",
      "  (2, 10)\t0.208572851013354\n",
      "  (2, 5)\t0.5222825618037251\n",
      "  (2, 4)\t0.3311691914996199\n",
      "  (3, 24)\t0.33944982039617033\n",
      "  (3, 22)\t0.44865823615762696\n",
      "  (3, 20)\t0.44865823615762696\n",
      "  (3, 10)\t0.2137880534581112\n",
      "  (3, 3)\t0.535341832365254\n",
      "  (3, 0)\t0.38715524587336536\n",
      "  (4, 24)\t0.3681610266939738\n",
      "  (4, 23)\t0.5806218969442964\n",
      "  (4, 20)\t0.48660646414754405\n",
      "  (4, 14)\t0.48660646414754405\n",
      "  (4, 10)\t0.23187058742344932\n",
      "  (5, 25)\t0.5222825618037251\n",
      "  (5, 24)\t0.3311691914996199\n",
      "  (5, 17)\t0.5222825618037251\n",
      "  (5, 14)\t0.4377135482191675\n",
      "  (5, 10)\t0.208572851013354\n",
      "  (5, 4)\t0.3311691914996199\n",
      "  (6, 16)\t0.4621273957203346\n",
      "  (6, 13)\t0.4621273957203346\n",
      "  (6, 10)\t0.18454996491533093\n",
      "  (6, 9)\t0.4621273957203346\n",
      "  (6, 1)\t0.4621273957203346\n",
      "  (6, 0)\t0.33420710786683616\n",
      "  (7, 18)\t0.5211099857546339\n",
      "  (7, 12)\t0.5211099857546339\n",
      "  (7, 10)\t0.20810458431736414\n",
      "  (7, 6)\t0.5211099857546339\n",
      "  (7, 0)\t0.37686287987345346\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在(index1,index2)中：index1表示為第幾個句子或者文檔，index2為所有語料庫中的單詞組成的詞典的序號，之後的數字為該詞所計算得到的TF－idf的結果值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 TfidfVectorizer\n",
    "\n",
    "TfidfTransformer和TfidfVectorizer\n",
    "具體來說，TfidfTransformer的作用已經不是在特徵提取了，而是特徵加權。而文本特徵權重的計算方法有許多，scikit-learn貌似也只提供了TF-TDF這一種。\n",
    "\n",
    "關於TfidfTransformer給出的例程尤其簡單，這裡就不進行講述。\n",
    "\n",
    "而TfidfVectorizer則是CountVectorizer和TfidfTransformer的結合版本。\n",
    "\n",
    "一般在文本特徵處理過程中，本來正常的流程就是先用CountVectorizer來提取特徵，然後就用TfidfTransformer來計算特徵的權重。而TfidfVectorizer則是把兩者的功能合在一起，連參數也都是兩者的參數合在一起，所以可以方便的直接使用TfidfVectorizer。但是如果想在CountVectorizer來提取特徵後想處理特徵，比如降維之類的，這樣直接使用TfidfVectorizer就不行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:\n",
      "['上海', '什麼', '今天', '北京', '在哪', '天氣', '好不好', '好玩的', '幫我', '怎麼樣', '找找', '搜索', '新聞', '明天', '有沒有', '東方明珠', '查下', '查看', '查詢', '火車', '特朗普', '看看']\n",
      "22\n",
      "Vectorized corpus:\n",
      "[[0.         0.         0.         0.3183848  0.         0.42081614\n",
      "  0.         0.         0.20052115 0.50212047 0.         0.\n",
      "  0.         0.50212047 0.         0.         0.42081614 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.50212047 0.3183848  0.         0.42081614\n",
      "  0.50212047 0.         0.20052115 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.42081614 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.42141948 0.         0.\n",
      "  0.         0.         0.26541316 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.6646151  0.55699932 0.         0.        ]\n",
      " [0.50057382 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27641806 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.58009435\n",
      "  0.         0.58009435 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.24938702 0.         0.         0.\n",
      "  0.52336667 0.         0.         0.         0.         0.52336667\n",
      "  0.         0.         0.62448441 0.        ]\n",
      " [0.         0.         0.         0.35097418 0.         0.\n",
      "  0.         0.         0.22104618 0.         0.         0.\n",
      "  0.46389023 0.         0.55351674 0.         0.         0.\n",
      "  0.         0.         0.         0.55351674]\n",
      " [0.37686288 0.52110999 0.         0.         0.         0.\n",
      "  0.         0.52110999 0.20810458 0.         0.         0.52110999\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.37686288 0.         0.         0.         0.52110999 0.\n",
      "  0.         0.         0.20810458 0.         0.52110999 0.\n",
      "  0.         0.         0.         0.52110999 0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "index of `什麼` is : 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# step 1\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "#vectorizer = TfidfVectorizer(stop_words=stpwrdlst, sublinear_tf=True, max_df=0.5)\n",
    "\n",
    "'''\n",
    "# step 2\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# step 3\n",
    "bag_of_words = vectorizer.get_feature_names()\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_words)\n",
    "print(len(bag_of_words))\n",
    "\n",
    "# step 4\n",
    "X = vectorizer.transform(corpus)\n",
    "print(\"Vectorized corpus:\")\n",
    "print(X.toarray())\n",
    "'''\n",
    "\n",
    "# Step 4.5\n",
    "# Step 2&3&4 can be combined into Step 4.5\n",
    "# fit + transform = fit_transform\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "bag_of_words = vectorizer.get_feature_names()\n",
    "print(\"Bag of words:\")\n",
    "print(bag_of_words)\n",
    "print(len(bag_of_words))\n",
    "print(\"Vectorized corpus:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# step 5\n",
    "print(\"index of `什麼` is : {}\".format(vectorizer.vocabulary_.get('什麼')))\n",
    "\n",
    "# step 6\n",
    "corpus_vecs = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x22 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfTransformer和TfidfVectorizer\n",
    "\n",
    "具體來說，TfidfTransformer的作用已經不是在特徵提取了，而是特徵加權。而文本特徵權重的計算方法有許多，scikit-learn貌似也只提供了TF-TDF這一種。\n",
    "\n",
    "關於TfidfTransformer給出的例程尤其簡單，這裡就不進行講述。\n",
    "\n",
    "而TfidfVectorizer則是CountVectorizer和TfidfTransformer的結合版本。\n",
    "\n",
    "一般在文本特徵處理過程中，本來正常的流程就是先用CountVectorizer來提取特徵，然後就用TfidfTransformer來計算特徵的權重。而TfidfVectorizer則是把兩者的功能合在一起，連參數也都是兩者的參數合在一起，所以可以方便的直接使用TfidfVectorizer。但是如果想在CountVectorizer來提取特徵後想處理特徵，比如降維之類的，這樣直接使用TfidfVectorizer就不行了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 儲存 TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://stackoverflow.com/questions/29788047/keep-tfidf-result-for-predicting-new-content-using-scikit-for-python\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tf-idf based vectors\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), stop_words = \"english\", lowercase = True, max_features = 500000)\n",
    "\n",
    "# Fit the model\n",
    "tf_transformer = tf.fit(corpus)\n",
    "\n",
    "# Dump the file\n",
    "pickle.dump(tf_transformer, open(\"tfidf1.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "# Testing phase\n",
    "tf1 = pickle.load(open(\"tfidf1.pkl\", 'rb'))\n",
    "\n",
    "# Create new tfidfVectorizer with old vocabulary\n",
    "tf1_new = TfidfVectorizer(analyzer='word', ngram_range=(1,2), stop_words = \"english\", lowercase = True,\n",
    "                          max_features = 500000, vocabulary = tf1.vocabulary_)\n",
    "X_tf1 = tf1_new.fit_transform(new_corpus)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tfidf/tfidf.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save your model in disk\n",
    "joblib.dump(vectorizer, 'data/tfidf/tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your model\n",
    "vectorizer = joblib.load('data/tfidf/tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x22 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"我 想 查 去 北京 的 車票\"\n",
    "\n",
    "vectorizer.transform([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 計算相似度 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sim_paragraphs(query_vec, paragraph_vecs, limit=5):\n",
    "    ''' Get top X paragrahs that similar to query\n",
    "        input: narray\n",
    "        output: \n",
    "    '''\n",
    "    match_scores = []; match_paragraphs = []\n",
    "    \n",
    "    # compute simple dot product as score\n",
    "    scores = np.sum(query_vec * paragraph_vecs, axis=1)\n",
    "    topk_idx = np.argsort(scores)[::-1][:limit]\n",
    "    for idx in topk_idx:\n",
    "        print('> %s\\t%s\\t%s' % (idx, scores[idx], corpus[idx]))\n",
    "        match_scores.append(scores[idx])\n",
    "        match_paragraphs.append(corpus[idx])\n",
    "    return match_scores, match_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "query = \"我 想 查 去 北京 的 車票\"\n",
    "query_vec = vectorizer.transform([query])[0]\n",
    "print(type(query_vec))\n",
    "\n",
    "query_vec = query_vec.toarray()\n",
    "print(type(query_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2\t0.42141947906854116\t幫我 查詢 去 北京 的 火車\n",
      "> 5\t0.3509741767018308\t幫我 看看 有沒有 北京 的 新聞\n",
      "> 1\t0.3183848028404604\t幫我 查下 今天 北京 天氣 好不好\n",
      "> 0\t0.3183848028404604\t幫我 查下 明天 北京 天氣 怎麼樣\n",
      "> 7\t0.0\t幫我 找找 上海 東方明珠 在哪\n",
      "CPU times: user 698 µs, sys: 576 µs, total: 1.27 ms\n",
      "Wall time: 694 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "match_scores, match_paragraphs = get_sim_paragraphs(query_vec, corpus_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42141947906854116, 0.3509741767018308, 0.3183848028404604, 0.3183848028404604, 0.0]\n",
      "['幫我 查詢 去 北京 的 火車', '幫我 看看 有沒有 北京 的 新聞', '幫我 查下 今天 北京 天氣 好不好', '幫我 查下 明天 北京 天氣 怎麼樣', '幫我 找找 上海 東方明珠 在哪']\n"
     ]
    }
   ],
   "source": [
    "print(match_scores)\n",
    "print(match_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://my.oschina.net/u/2293326/blog/1838918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
